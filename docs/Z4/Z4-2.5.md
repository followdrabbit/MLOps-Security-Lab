# Z4-2.5 — Adversarial & Security Testing (**pré-registro**)

*último “checkpoint de risco” antes de qualquer modelo entrar no Registry (Z5)*

Esta etapa **ataca o seu próprio modelo** de forma controlada para medir:

1. **robustez** (evasion/backdoor),
2. **privacidade** (membership/model inversion/extraction),
3. **integridade/uso seguro** (limites de saída, políticas),
4. **LLM-specific** (prompt injection, jailbreak, exfiltração).

Se qualquer teste crítico **falhar**, o **pipeline é bloqueado** e o artefato **não** segue para a Z5.

---

## 1) Objetivo & Escopo

* **Objetivo:** certificar que o candidato a produção **resiste a ataques realistas** e **não vaza informação** indevida diante de adversários com orçamento limitado (queries/perturbação).
* **Escopo por tipo de modelo:**

  * **Tabular/Clássico (XGBoost/GBM/Regressão/Arvores)**
  * **CV/NLP clássicos (CNN/RNN/Transformer não-instruído)**
  * **LLM/GenAI** (instruído/afinados, RAG, ferramentas)

---

## 2) Taxonomia de ameaças cobertas

* **Dados/treino:** *data poisoning* (flip/clean-label), **backdoors** (gatilhos), *label noise* malicioso.
* **Inferência:** *evasion* (FGSM/PGD, perturbações pequenas), *transfer attacks*.
* **Privacidade:** *membership inference (MI)*, *model inversion*, *property inference*, **model extraction** (stealing).
* **Integridade de saída:** limites numéricos/semânticos, saturação, *score collapse*.
* **LLM/GenAI:** **prompt injection**, *jailbreak*, **exfiltração de contexto** (RAG/tools), **conteúdo inseguro** (políticas).

---

## 3) Metodologia (como testar)

1. **Congelar candidato**: `git_sha`, `container_digest`, *seed*, *dataset snapshot* (Z3).
2. **Fixar orçamento do adversário**: `ε` (norma L∞/L2), #queries, tempo por ataque.
3. **Execução batelada** por família de ataque (abaixo), com *fixtures* reproduzíveis.
4. **Métricas + gates**: queda aceitável de métrica, risco de privacidade, violação de política.
5. **Relatórios & evidências** anexados ao **MLflow** + **Model Card**.

---

## 4) Famílias de teste (o que rodar de fato)

### 4.1 Evasion / Robustez

* **Ataques:** FGSM/PGD (tabular/CV), *word-level perturb* (NLP), *AutoAttack* (quando aplicável).
* **Métricas:** ΔAUC/ΔAccuracy ≤ **X%** sob `ε` pré-definido; taxa de confusão direcionada.
* **Defesas avaliadas:** normalização, *adversarial training*, *feature squeezing*, *input filters*.

### 4.2 Backdoors / Gatihos

* **Detecções:** *Spectral Signatures*, *Activation Clustering*, **STRIP** (consistência).
* **Procedimento:** procurar *clusters* anômalos no espaço de ativações; avaliar disparo do gatilho.
* **Gate:** *no trigger found* em amostra ampla ou **risco muito baixo** + justificativa/mitigação.

### 4.3 Data Poisoning (limpo/sujo)

* **Sinais:** *influence functions*, *loss landscape* anômalo, “bursts” de padrões em Z3.
* **Gate:** % de exemplos com influência extrema ≤ limiar; **sem** *concept flip* direcionado.

### 4.4 Membership Inference (MI)

* **Ataques:** *shadow models/LiRA* (probabilidade de pertença).
* **Métrica:** *attack AUC* ≤ **0.55** (exemplo) e risco classificado como **LOW**.
* **Defesas avaliadas:** *regularização*, *dropout*, *DP-SGD* (quando aplicável), limitação de *confidence*.

### 4.5 Model Inversion / Property Inference

* **Testes:** reconstrução aproximada de atributos sensíveis a partir de *gradients/outputs*.
* **Gate:** qualidade de reconstrução **abaixo** de limiar (p.ex., PSNR/acc), com *hard caps*.

### 4.6 Model Extraction (Stealing)

* **Cenário:** orçamento de N mil queries; treinar *surrogate* e medir similaridade (fidelidade/top-k).
* **Gate:** fidelidade do *surrogate* < **X%** sob limites de rate/ruído; caso contrário → endurecer **Z6**.

### 4.7 Output Integrity (limites/semântica)

* **Tabular:** limites de score (0–1), monotonicidade esperada, *no NaN/inf*, distribuição plausível.
* **Gate:** violações **0**; *score collapse* detectado → **FAIL**.

### 4.8 Testes **LLM/GenAI** (se aplicável)

* **Prompt Injection/Jailbreak:** *baterias* de *prompts* maliciosos.
* **Exfiltração de contexto (RAG/tools):** pedindo explicitamente dados internos/PII.
* **Safety/Policy:** classificação por *policy engine* (ódio, violência, PII, sexual, etc.), *structured output* (JSON Schema), *toxicity score*.
* **Gates:** taxas de violação **≤ limiar**; *guardrails* (input/output filtering) **obrigatórios** no *serving* (Z6).

---

## 5) Métricas & **Gates de aprovação**

| Família          | Métrica de decisão                  | Gate sugerido (exemplo)        |
| ---------------- | ----------------------------------- | ------------------------------ |
| Evasion          | ΔAUC/ΔF1 com `ε` fixo               | ≤ 5% de degradação             |
| Backdoor         | *Trigger success rate*              | 0% no *holdout* ampliado       |
| Poisoning        | % pontos de alta influência         | ≤ 0.5%                         |
| MI               | *Attack AUC*                        | ≤ 0.55 (LOW)                   |
| Inversion        | Qualidade de reconstrução           | abaixo de limiar conservador   |
| Extraction       | Fidelidade do *surrogate*           | < 70% sob N queries            |
| Output Integrity | # violações (NaN/intervalo/etc.)    | 0                              |
| LLM Safety       | % respostas em violação de política | ≤ 1% (e **zero** PII em claro) |

> **Políticas vivem versionadas** (`security_gates.yml`). Valores devem ser ajustados ao contexto/regulatório.

---

## 6) Evidências **obrigatórias** (artefatos do run)

* `adv_eval.json` (resumo por ataque, parâmetros, *budgets*).
* `privacy_eval.json` (MI/inversion/extraction com risco).
* `robustness_curves/*.png` (performance × `ε`).
* `backdoor_report.json` (clusters/assinaturas).
* `output_integrity.json` (checagens numéricas/semânticas).
* Para LLM: `llm_redteam.csv` (prompt→output→veredito), `policy_violations.json`.
* **Conclusão:** `security_decision.txt` (**PASS/FAIL** + justificativas).
* Referenciados no **Model Card** (secção Segurança/Privacidade).

---

## 7) Integração no pipeline (com Z4-2.3)

```yaml
# Trecho de DAG (pseudo)
- name: adversarial-security-tests
  needs: ["train","hygiene"]
  image: registry.corp/ml-test@sha256:...
  env:
    MODEL_URI: "mlflow://runs/<run_id>/artifacts/model"
    DATA_SNAPSHOT: "z3://curated/.../snapshot=2025-11-01"
    GATES_FILE: "security_gates.yml"
  run: |
    python tests_sec/run_evasion.py --eps 0.01 0.02 0.04
    python tests_sec/run_backdoor.py --search spectral,ac
    python tests_sec/run_mi.py --budget 50000
    python tests_sec/run_inversion.py --time 10m
    python tests_sec/run_extraction.py --queries 20000
    python tests_sec/run_output_integrity.py
    python tests_sec/run_llm_battery.py --policy bank.yaml
    python tests_sec/decide.py --gates security_gates.yml
```

> O *step* `decide.py` grava `security_decision.txt`. **Só se PASS** o pipeline chama “**assinar & submeter**” (Z4-2.7 → Z5).

---

## 8) Exemplos rápidos (didáticos)

**FGSM (conceito)**

```python
# x: input, y: label; model: rede; eps: perturbação
x.requires_grad_(True)
loss = criterion(model(x), y)
loss.backward()
x_adv = x + eps * x.grad.sign()      # perturbação de sinal
pred_drop = metric(model(x_adv), y)  # medir queda de métrica
```

**Membership Inference (esqueleto)**

```python
# Treina shadow, coleta confianças (in/out), treina atacante:
att_X, att_y = collect_confidences(model, data_in, data_out)
att = LogisticRegression().fit(att_X, att_y)
auc = roc_auc_score(att_y, att.predict_proba(att_X)[:,1])
risk = "LOW" if auc <= 0.55 else "HIGH"
```

**LLM Guard (estrutura)**

```python
res = llm.generate(prompt)
if violates_policy(res, policy="bank.yaml"): fail("safety")
if leaks_pii(res): fail("pii")
if not validate_json_schema(res): fail("schema")
```

---

## 9) Defesas avaliadas / recomendações (quando reprovar)

* **Evasion:** *adversarial training*, *input normalization*, *randomized smoothing*, *feature squeezing*.
* **Backdoor:** *fine-pruning*, *activation repair*, re-treino sem suspeitos; **verificação de dados** na Z3.
* **Poisoning:** *robust loss*, filtros de outliers/duplicados; reforçar **Z0–Z3** (contratos/DQ).
* **MI/Inversion:** **DP-SGD**, limitar *confidence/logits*, *temperature*, *output clipping*.
* **Extraction:** **rate limit**, *jitter* nos outputs, *distillation traps* (em produção, Z6).
* **LLM:** *system prompt* fixo, **input/output filtering**, *tool sandboxing*, **RAG com filtro de contexto** (PII/segredos), validação por *policy engine* e **JSON Schema**.

---

## 10) Riscos × Controles × Frameworks

| Risco                      | Controles (esta etapa)         | Frameworks                                                |
| -------------------------- | ------------------------------ | --------------------------------------------------------- |
| Data Poisoning / Backdoor  | 4.2–4.3 + bloqueio de promoção | OWASP ML Top 10 (ML02/ML10), NIST AI RMF (Measure/Manage) |
| Evasion / Robustez fraca   | 4.1 + curvas de robustez       | OWASP ML, ISO/IEC 23894 (risco em IA)                     |
| Membership / Inversion     | 4.4–4.5 + limites de confiança | OWASP ML (privacidade), LGPD/GDPR                         |
| Model Extraction           | 4.6 + limites de queries       | OWASP API A04/A10, CSA AICM (IAM/SEF)                     |
| Output Integrity           | 4.7 (sanidade/limites)         | NIST SP 800-53 SI-*, AU-*, CSA LOG                        |
| LLM Prompt Injection/Exfil | 4.8 (bateria + policies)       | OWASP LLM/GenAI Top 10                                    |

---

## 11) Runbooks (quando dá **FAIL**)

* **Backdoor/Evasion detectado:** re-treinar com *adversarial training*; revisar dados; registrar SCDR/ADR com decisão.
* **MI alto:** aplicar **DP-SGD** / calibrar *confidence*; repetir teste; reavaliar *trade-off* métrica × privacidade.
* **Extraction alto:** ajustar políticas de **Z6** (rate, authz fina, ruído controlado), *caching* seguro; repetir teste sob novos limites.
* **LLM violando políticas:** reforçar *prompt system*, *output filters*, *tool rules*; revisar base RAG e PII masking.

---

### Frase pronta para a entrevista

> “**Nenhum modelo entra no Registry sem passar por Adversarial & Security Testing.** Rodamos *evasion, backdoor, poisoning, MI/inversion/extraction, output integrity* e, para LLMs, *prompt-injection/jailbreak/exfiltração*. Tudo é versionado, com *budgets* de ataque claros, **gates de aprovação**, e evidências anexadas ao **Model Card**. Se falhar, o pipeline bloqueia e abrimos SCDR/ADR com mitigação.”
